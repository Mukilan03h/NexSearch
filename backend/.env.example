# =============================================================================
# AI Research Assistant - Environment Configuration
# =============================================================================

# --- LLM Configuration (LiteLLM) ---
# Primary provider: openai, anthropic, ollama
LLM_PROVIDER=openai
OPENAI_API_KEY=your_openai_api_key_here
MODEL_NAME=gpt-4-turbo-preview
EMBEDDING_MODEL=text-embedding-3-small

# Alternative: Anthropic
# LLM_PROVIDER=anthropic
# ANTHROPIC_API_KEY=your_anthropic_key
# MODEL_NAME=claude-3-sonnet-20240229

# Ollama (local LLM, set ENABLE_OLLAMA=true)
ENABLE_OLLAMA=false
OLLAMA_HOST=http://ollama:11434
OLLAMA_MODEL=llama3.2

# LLM Parameters
TEMPERATURE_PLANNING=0.0
TEMPERATURE_WRITING=0.7
MAX_TOKENS=4000

# --- PostgreSQL ---
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=research_assistant
POSTGRES_USER=research_user
POSTGRES_PASSWORD=research_pass_2026

# --- MinIO (S3-compatible) ---
MINIO_ENDPOINT=minio:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin123
MINIO_SECURE=false
MINIO_BUCKET_PAPERS=papers
MINIO_BUCKET_REPORTS=reports

# --- Vespa ---
VESPA_HOST=http://vespa
VESPA_PORT=8080
VESPA_DEPLOY_PORT=19071

# --- Data Sources ---
ARXIV_MAX_RESULTS=20
ARXIV_DELAY_SECONDS=3.0
SEMANTIC_SCHOLAR_API_KEY=
ENABLE_SEMANTIC_SCHOLAR=false
ENABLE_PUBMED=false
ENABLE_OPENALEX=false

# --- Application ---
LOG_LEVEL=INFO
MAX_PAPERS_PER_QUERY=15
TOP_K_PAPERS=10
ENABLE_CACHING=true
CACHE_TTL_HOURS=24

# --- API ---
API_HOST=0.0.0.0
API_PORT=8000
